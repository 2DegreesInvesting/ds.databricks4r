---
output: github_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Databricks for RStudio users

The goal of this series is to help R users understand if and how they can do in
Databricks the things they commonly do in RStudio.

At the end of this series you'll be able to do things like this:

* Add folders, .R files and notebooks to your Workspace.
* Connect to an existing cluster.
* Work with Git and GitHub.
* Create a dataframe from a .csv file in the file system.
* Create a spark dataframe from a table in the Databricks Catalog.
* Create and run jobs.
* Create a cluster with RStudio installed in it.

## Who is the audience?

Analysts used to RStudio.

## Why is this important?

Databricks allows you to work with data at scale from a simple interface. With
that simple interface you can do powerful things, but not all the things you
typically do in RStudio, and not always in the way you're used to. For example,
sometimes you need to read, and manipulate data in a different way.
Understanding what you can and shouldn't do with Databricks will save you time
and frustration.

## Workspace

* Add a folder, file.R and notebook with markdown
* Connect to a running cluster
```
%md 
# hi
```

```
%r
print("hi")
```

* In the file.R note the Output and Terminal tabs

**Friction**

* Editing text is limited.

* Only a few file formats are supported. Try bad.sh
```bash
# Fails
echo "hi"
```

* The working directory seems to vary with the language:
```
%r
getwd()
[1] "/tmp/Rserv/conn4266"
```

```
%sh
Rscript -e "getwd()"
[1] "/databricks/driver"
```

```
%python
os.getcwd()
[1] "/databricks/driver"
```

## Repos

* Cone https://github.com/2DegreesInvesting/ds.databricks4r
* Change README on a new branch.
* Commit and push.
* Create a PR on GitHub.

**Friction**

* Git operations from the UI are basic, and from the Terminal may be impossible:
```bash
/Workspace/Repos/mauro@2degrees-investing.org/ds.databricks4r# git status
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
```

## Catalog

* Read a (parquet?) file from the Databricks Catalog.

<image width=700 src=https://github.com/2DegreesInvesting/ds.databricks4r/assets/5856545/a632de43-c83e-41be-9577-3b7c89019156>

```r
path <- "raw.default.country"
data <- SparkR::tableToDF("raw.default.country")
data
```

* Read a .csv file from "disk".

```r
path <- "/databricks-datasets/Rdatasets/data-001/datasets.csv"
data <- readr::read_csv(path)
data
```

* Read a .csv file from Azure storage (works outside Databricks).

<img width=700 src=https://github.com/2DegreesInvesting/tiltIndicator/assets/5856545/f8aa5a3e-ebff-48e6-87eb-889f477c8831>

```r
library(AzureStor)

# usethis::edit_r_environ()
# User delegation key with all 10 permissions
# AZURE_CONTAINER_SAS_TEST_MAURO="paste the SAS token here"
url <- "https://storagetiltdevelop.blob.core.windows.net/test-mauro"
sas <- Sys.getenv("AZURE_CONTAINER_SAS_TEST_MAURO")
container <- blob_container(url, sas = sas)

storage_read_csv(container, "data/iris.csv")
```

**Friction**

* SparkR is no longer on CRAN [(archived in 2021 for lack of maintenance)](https://cran.r-project.org/web/packages/SparkR/index.html).
* The sparklyr package seem like a good alternative but I failed to use it.
* Code written for R dataframes may not work for [spark dataframes](https://spark.apache.org/docs/latest/sparkr.html#sparkdataframe-operations).
* The [sparklyr package](https://spark.rstudio.com/deployment/databricks-cluster.html) may help but I failed to configure and use it.

## Workflows

* Create a job that writes each column in a dataset to a .csv file (an example).

```r
data <- cars
cols <- names(data)

for (i in seq_along(cols)) {
  path <- paste0(cols[i], ".csv")
  message("Writing ", path, " in ", getwd())
  readr::write_csv(data[i], path)
}
```

* Create a workflow, run it and view it.

## Compute

* Create a new cluster with RStudio (use Runtime ML, dissable auto termination).
* Try to use it to run a previous file. Note it takes time to start.
* Setup RStudio: Compute > click on cluster > Apps > Set up RStudio.

**Friction**

* Starting a cluster takes minutes.
* RStudio lacks `sudo`.
* RStudio requires no auto-termination.
* RStudio lacks direct access to the Databricks Catalog.
* RStudio won't persist when restarting the cluster.

## Resources

* YouTube [playlist](https://bit.ly/ds-incubator-videos).
* [What is Databricks](https://docs.databricks.com/en/introduction/index.html).
* [Video (46')](https://youtu.be/VokwQdJWUas?si=Eb1_KH7smZzxUx8q)

## A note on friction

I'm new to Databricks and that alone may explain many frictions.

## Thanks

Tanks Sven for patiently answering my questions.
